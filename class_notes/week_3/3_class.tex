\documentclass[12pt]{article}

\usepackage[paper=letterpaper,margin=2.5cm]{geometry} % Set Margins

%% Math and math fonts
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{bbold} % for \mathbbm{1}

% date
\usepackage[nodayofweek]{datetime}

% Color
\usepackage{color, xcolor}

% Misc
\usepackage{environ}  % \collect@body in asmmath
\usepackage{graphicx} % \includegraphics options
\usepackage{mdframed} % text boxes
\usepackage{indentfirst} % Indent first paragraph after section header
\usepackage[shortlabels]{enumitem} % Control enumerate items with [(a)]
\usepackage{comment} % Comments
\usepackage{fancyhdr} % Headers and footers

% Tables
\usepackage{array}

% Sub-figures and figure placement
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float} 

% Graphing
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{tikz}

% Title Placement
\usepackage{titling}
\setlength{\droptitle}{-6em}

%set indent to 
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt}

% Hyper refs
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor  = blue,
    filecolor=magenta,      
    urlcolor=blue,
    citecolor = blue,
    anchorcolor = blue
}

% % Citation management
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authordate,open={(},close={)}}

% ----------------------------------------
% TITLE
% ----------------------------------------

\pagestyle{fancy}

\lhead{Creel}
\chead{Week Three}
\rhead{FAME}

\title{FAME Week Three Class Notes -- Derivatives II}
\author{Andie Creel}

\begin{document}
\maketitle

\section{Maximum and minimum}
If you're out hiking, how do you know if you're at the top of a hill? You will walk on flats at the top. We often want to know the peaks and valleys of slopes. When are we at the top? When are we at the bottom? 

In applied setting we frequently want to know when we can maximize an objective or minimize an objective? How much time does a critter spend vigilant to maximize its chance of survival? How much money should a business invest in climate mitigation strategies to maximize their profit? 

These questions are interesting on their own. These are the questions that motivate \textit{why} we need to learn calculus, because these questions can be answered \textit{using} calculus. Calculus isn't important on its own, it's important because it can help us answer important questions. 

\subsection{How do we know if we're going to a peak or a valley?}
We can use calculus to find a peak or find a valley. Consider a mountain range that can be modeled with the equation
\begin{align*}
    Y = F(x) = ax - bx^2 \\
    a > 0 \text{ and } b > 0
\end{align*}
where is a peak/valley of this mountain range? At what level of $x$ is $F(x)$ maximized or minimized? We can see when the derivative of the function equals zero, then solve for $x$. This value of $x$ tells us where the function is ``flat''.

\begin{align*}
    \frac{dF(x)}{dx} &= a - 2bx = 0 \implies \\
    x &= \frac{a}{2b}
\end{align*}

How do we know if $x = a/(2b)$ this is a maximum or minimum (peak or valley)? We can use the \textbf{second derivative}. If the second derivative is negative, it's a maximum. If the second derivative is positive, it's a minimum. (Use Eli's tricks of smiley faces and frowny faces to remember this. If the second derivative is negative, the function is a frowny. If the second derivative is positive, the function is a smiley). 

A function is concave is the second derivative is negative. A function is convex if the second derivative is positive.

\begin{align*}
    \frac{d^2F(x)}{dx^2} = -2b.
\end{align*}
Assuming $b>0$ implies $-2b<0$ therefore we have a local maximum because the second derivative is negative.

Physicists do a good job keeping of functions, first derivatives and second derivatives. It's (respectively) location, speed, acceleration, jerk (``if someone's being really mean to you, you can say you're being a really big third derivative'' - Eli). 

\section{Deriving the equation for taking the mean/average}

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{mean_pic.png}
    \caption{Dotted line is mean} 
    \label{fig:x_bar}
\end{figure}

How do we find an average? We want to find a constant, $\bar x$, that minimizes the total distance of all $x_i$ values from $\bar x$, squared. Why are we squaring it? We don't care if $x_i$ is greater that $\bar x$ or less than $\bar x$. We just want to find a $\bar x$ where all $x_i$ are not that far away from $\bar x$.

Figure \ref{fig:x_bar} shows all the $x_i$ points as the dots and the dashed line as $\bar x$. Notice that the y-axis is $x$, and the x-axis is just showing us different observations, which we denote at $i$.

We can define the mean as the constant $\bar x$ that minimizes the differences between itself and all other $x_i$ observations,

\begin{align*} 
    \bar x = \textbf{argmin} \sum_i^N (x_i - \bar x) ^2
\end{align*}
Great, we have an equation defining the mean. However, this is not very easy to compute (how do you use an argmin?? IDK!). It's defined clearly, but we can't calculate it easily. 


\subsection{Deriving eqn for mean}

\textbf{Objective:} Find the $\bar x$ that minimizes the distance from itself and all other $x_i$, where the distance between two points $x,y$ is measured as $(x - y)^2$. 

To achieve this objective, we first need a function that measures the total distance of all $x_i$ and $\bar x$. That function is written as:
\begin{align}
    F(\bar x) &= \sum_i^N (x_i - \bar x)^2\\
    &= (x_1 - \bar x)^2 + (x_2 - \bar x)^2  + (x_3 - \bar x)^2 +... + (x_N - \bar x)^2 \label{eqn:sum_dist}
\end{align}

If we're wanting to \textit{minimize} (getting back to the argmin) the distance from $x_i$ to $\bar x$, we can take a derivative of Equation \ref{eqn:sum_dist} and set it equal to zero: 
\begin{align*}
    \frac{d F(\bar x)}{d\bar x} &= -2(x_1 - \bar x)  -2(x_2 - \bar x) -2(x_3 - \bar x) - ... -2(x_N - \bar x) = 0\\
\end{align*}
We can simplify by dividing both sides by $-2$, then group the $\bar x$ into one term, 
\begin{align}
    x_1 + x_2 + x_3 + ... x_n - N \bar x &= 0\\
    x_1 + x_2 + x_3 + ... x_n &= N \bar x \label{eqn:series}
\end{align}
simplify the left-hand side of \ref{eqn:series} by changing it back to summation notation 
\begin{align}
    \sum_i^N x_i = N \bar x \implies \\
    \bar x = \frac{1}{N}\sum_i^N x_i
\end{align}

This is how the \textbf{arithmetic mean} was derived. When someone wanted to find an $\bar x$ that was not that different from all the other $x_i$, they solved this problem! 

To confirm that $\bar x$ \textit{minimizes} the distances between all $x_i$ and itself, you can take a second derivative and confirm that the second derivative is positive, which means the function is convex and we've found the minimum of that convex function.  

\subsection{Problem set hint}
The hardest problem on the problem set is a challenge problem asking us to derive the process for linear regression. 

Linear regression finds a conditional mean \textit{aka} a mean conditioned on $x$, which is just a line. 

In the problem set, we will want to minimize the distance between $y_i$ and $F(x_i)$,
\begin{align}
    \text{distance}=(y_i - F(x_i))^2.
\end{align}
What's $F(x)$ going to be? The previously mentioned conditional mean, \textit{i.e.,} a line. In the problem set, we parameterize a line that minimizes the distance between the data points and the line. Solving this problem solves for the same algorithms that computers use when they fit linear regressions. 

\section{Mean Value Theorem}
\textbf{Definition of Mean Value Theorem}: For a smooth and continuous function, there is at least one point on a given interval where the derivative is equal to the average rate of change over the interval. 

\textbf{Intuition:} The theorem says that if you draw the secant line (the straight line) connecting the point $(a, f(a))$ (where $a$ is on the x-axis and $f(x)$ is on the y-axis) and the point $(b, f(b))$, then somewhere in the interval $(a,b)$ there will be a point $c$ where the tangent line (the line that touches the curve at exactly one point and has the same slope as the curve at that point) is parallel to the secant line. In other words, at some point in the interval, the function's derivative matches the average rate of change over the interval, 
\begin{align}
    f'(c) = \frac{f(b) - f(a)}{b - a}.
\end{align}

\section{Taylor Series: A Polynomial Approximation}

We know that straight lines are good approximators because a line is a conditional mean, and means are good at minimizing error.

Series are good approximators of sequences of numbers, because series are functions. 

A \textbf{Taylor Series} is an extremely useful series for approximating the relationship of sequence of numbers (data is a sequence of numbers). Taylor series underpins a lot of the math we do today, particularly for any relationship that isn't linear. 

\begin{align}
    f(x) = f(a) + \frac{f'(a)}{1!}(x-a)^1 + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + ... + \frac{f^k(a)}{k!}(x-a)^k + R
\end{align}
where $R$ is the residual, or higher order terms.


\subsection{0th order Taylor Series}
Consider a 0th order taylor series: it would be a \textbf{mean} aka just a flat line equal to $f(a)$. 

If you're only looking at the means of a dataset, that means you're doing a 0th order taylor approximation.

\subsection{First order Taylor Series}

Consider a first order taylor series: 
\begin{align}
    f(x) &= f(a) + \frac{f'(a)}{1!}(x-a)^1 \\
    &= A + B (x-a) \\
    &= A -aB + Bx\\
    &= m + Bx
\end{align}
It's a \textbf{line} with a slope. A first order Taylor series is a linear approximation. Linear regression is a first order Taylor approximation. 

\section{Mean Value Theorem and Taylor Series}
Remember the Taylor series. A \textbf{Taylor Series} can approximate \textit{any continuous function $f(x)$}. The modeler decides if they wants to approximate $f(x)$ with a 0th order taylor series (mean), a 1st order taylor series (line), a 2nd order taylor series (parabola), a 3rd order taylor series (at this point, you better have a LOT of data!!). We can approximate $f(x)$ with our taylor series,
\begin{align}
    f(x) \approx f(a) + \frac{f'(a)}{1!}(x-a)^1 + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + ... + \frac{f^k(a)}{k!}(x-a)^k + R
\end{align}
where $R$ is the residual.\\

Remember that what the \textbf{Mean Value Theorem} tells us: If you have two points, the slope between those two points will be parallel to the tangent line of the original function. So, the taylor series is a large application of the mean value theorem, where a taylor series approximates for the slope at a point many many times. 

\subsection{Discussion of models}
Any Taylor Series approximation is a model. If someone says "I don't do models, let's only do means" they actually \textit{are} suggesting a model. A mean is a model, it's a zero order Taylor Series approximation! \\

Higher order Taylor Series will approximate an underlying function better than a lower order. \textit{However,} we do not always have enough data to fit a higher order Taylor series because it would require more estimating parameters and your data set may not have enough power (aka enough data) to do so. 


\subsection{Exercise: What is $\sqrt{10}$?}
What's is $\sqrt{10}$? The function we are considering is
\begin{align}
    f(x) = \sqrt{x} = x^{\frac{1}{2}}.
\end{align}

We're interested when $x = 10$. Let's do a \textbf{0th order taylor series approximation}: 

\begin{align}
    f(x) = f(a)
\end{align}

what should we choose $a$ to be? Let's choose $a = 9$ because 9 is close to 10. 

\begin{align}
    f(10) & \approx f(a) \\
    & \approx f(9)\\
    & \approx \sqrt{9}\\
    & \approx 3
\end{align}

So the 0th order approximation of $f(10) \approx f(9) = 3$. It's close! \\

Now, let's do a \textbf{first order taylor series approximation} 
\begin{align}
    f(x) &\approx f(a) + \frac{f'(a)}{1!}(x-a)^1. \label{taylor}
\end{align}

We need to find $f'(a)$,
\begin{align}
    f(a) = a^{1/2}\\
    f'(a) = \frac{1}{2} a^{-1/2} \label{f_prime}
\end{align}

We can plug \ref{f_prime} back into \ref{taylor} to get 

\begin{align}
    f(x) &\approx f(a) + \frac{1}{2} \frac{a^{-1/2}}{1} (x - a)^1
\end{align}

Now, let's find $f(10)$ where $x = 10$ by using the taylor expansion around $f(9)$ where $a = 9$,

\begin{align}
    f(10) &\approx f(9) +  \frac{f'(9)}{1!}(10-9)^1\\
    &\approx \sqrt{9} + \frac{1}{2}(9)^{-1/2}\times1\\
    &\approx 3 + \frac{1}{2}\frac{1}{(9)^{1/2}}\\
    &\approx 3 \frac{1}{6}
\end{align}

So our first order approximation of $f(10) \approx 3 \frac{1}{6}$. This is even closer than our original approximation, which was 3!\\

We can then do a \textbf{second order taylor series approximation} and get even closer.

\begin{align}
    f(x) &\approx f(a) + \frac{f'(a)}{1!}(x-a)^1 + \frac{f''(a)}{2!} (x-a)^2 \label{second_tay}
\end{align}

we already found what $f'(a)$ is. What's $f''(a)$? 
\begin{align}
    f(a) &= a^{1/2}\\
    f'(a) &= \frac{1}{2} a^{-1/2} \text{ (power rule)}\\
    f''(a) &= -\frac{1}{4} a^{-3/2} \text{ (power rule again)} \label{second_ord}
\end{align}

We can plug in \ref{second_ord} into \ref{second_tay} (along with what we found in our first order approximation) to get 
\begin{align}
     f(x) &\approx f(a) + \frac{1}{2} \frac{a^{-1/2}}{1} (x - a)^1 + \frac{1}{2} (-\frac{1}{4}) a^{-3/2} (x - a)^{2}
\end{align}
we know from our first order that the first two terms simplify to $3 \frac{1}{6}$. Lets simplify this expression 
\begin{align}
    f(10) &\approx 3 + \frac{1}{6} -\frac{1}{8}\frac{1}{9^{3/2}} (10 - 9)^2\\
    &= 3 + \frac{1}{6} -\frac{1}{8}\frac{1}{27}\\
    &= 3.16203703704
\end{align}
So what is $f(10) = \sqrt{10}$? $\sqrt{10} = 3.16227766017$. So we can see that our second order approximation got pretty close! 

\subsection{Example: Logistic growth}
Consider the logistic growth function: 
\begin{align}
    G(N) = rN (1 - \frac{N}{K})
\end{align}

What would we get if we "taylor expanded" this function? Let's work with the per capital growth rate
\begin{align}
    \frac{G(N)}{N} = r(1 - \frac{N}{K}) \label{per_cap}
\end{align}
and see if we can use a Taylor series that would approximate this right hand side equation

\begin{align}
    \frac{G(N)}{N} &\approx G(a) + G'(a) (N-a)\\
    &\approx G(a) + G'(a)N - G'(a)a \\
    & \approx G(a) - G'(a)a + G'(a) N
\end{align}

Let $r =  - G'(a)a$.
\begin{align}
    & \approx r + G(a) + G'(a) N
\end{align}

Let $G(a) = 0$  and rename $G'(a) = \frac{-r}{K}$

\begin{align}
    &\approx r + \frac{-r}{K}N\\
    &\approx r(1 - \frac{N}{K}) \label{approx}
\end{align}

And now we've shown that \ref{approx} is equivalent to \ref{per_cap}. Which means we've shown that we can derive the logistic growth equation using a Taylor series approximation.

\section{Partial derivatives}
Consider a function that maps from $R^2 \to R^1$, 
\begin{align}
    Z = F(x, y).
\end{align}

Consider a situation where we can change $x$ with policy, but we can't change $y$ and we want to maximize outcome $Z.$ In this case, we would take the derivative of $F(x,y)$ w.r.t $x$ (treat $y$ like a constant, assume it isn't changing). 

We can take a \textbf{partial derivative} which notation that are all equivalent: 
\begin{align}
    \frac{\partial F(x,y)}{\partial x} = F_x(x,y) = F_1(x,y)
\end{align}

This is how we find a \textbf{relative max/min}. Conditioned on $y$ staying constant, we can find the relative max/min (relative to that level of $y$). 

\subsection{Absolute max/min}
If you want to find the absolute maximum or minimum you need to take \textit{all} partial derivatives and set them all equal to zero and solve that system of equations. If your function mapped from $R^N \to R^1$ then you would take $N$ partial derivatives and solve the system of $N$ equations. 


\section{Total Derivatives}
Total derivative in a function is equal to the sum of the partials multiplied by the change. 

Consider the function $ F = f(x,y)$. We can get our partial derivatives. Recall that when we take a partial derivative wrt $x$, we're holding $y$ constant.
\begin{align}
    \frac{\partial f}{\partial x} = f_x\\
    \frac{\partial f}{\partial y} = f_y
\end{align}

Now consider how we would calculate the change in $F$ if both $x$ and $y$ changed. We'd need to get a full derivative, 

\begin{align}
    df &= f_x dx + f_y dy \\
    &= \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy\\
    \Delta f = &= \frac{\partial f}{\partial x} \Delta x + \frac{\partial f}{\partial y} \Delta y
\end{align}
These are all equivalent ways of writing the full derivative. 

\subsection{Example of maximizing a multi-variate function}
Consider the functions $F(x,y) = -x^2 -y^2$. Let's say that we want to maximize this function, and find the \textit{global} maximum. To do so, we need to take the partial derivatives and set them equal to zero, then solve for $x,y$. 

\begin{align}
    \frac{\partial F}{\partial x} = -2x = 0 \implies x = 0\\
    \frac{\partial F}{\partial y} = -2y = 0 \implies y = 0
\end{align}

So the maximum point of $F(x,y) = -x^2-y^2$ is $(0,0)$, where $x = 0 $ and $y = 0$. 

\subsection{Implicit Function Theorem (envelope theorem in econ)}

\begin{figure}[htp]
    \centering
        \includegraphics[width=0.5\textwidth]{Screen Shot 2023-09-27 at 10.59.32 AM.png}
    \caption{The f=0 represents an altitude}
    % \label{fig:sample}
\end{figure}

Now consider if you want to stay on one level set. For example, if you wan to stay on a contour line or on a utility level. That would mean we want the value of $f(x,y)$ to stay the same, $df = \Delta f = 0$, even when we change both the $x$ and $y$. We can set the full deriative to 0 in order to acheive this. 

\begin{align}
    d f = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy &= 0 \implies \\
    dy \frac{\partial f}{\partial y} dy &= - dx \frac{\partial f}{\partial x} \implies \\
    \frac{d y}{dx} &= \frac{- \frac{\partial f}{\partial x}}{\frac{\partial f}{\partial y}} \label{imp_func}
\end{align}

Equation \ref{imp_func} is the implicit function theorem. What equation \ref{imp_func} tells us is "if I want to stay at the original value of $f(x,y)$, and I change $x$, how do I need to change $y$ in order to stay at the original value of $f$". You could also think of this as you're hiking and you're at a certain altitude. The same questions would be "if I change my latitude, how do I need to change my longitude in order to stay at the same altitude?"\\


\textbf{The envelope theorem} is a special case of the implicit function theorem when first derivatives are set to zero. This leads to math simplifying because we can say that, at the optimum, a bunch of derivatives will equal zero. 

% \section{Taylor Series}

% We know that straight lines are good approximators because a line is a conditional mean, and means are good at minimizing error.\\

% Series are good approximators of sequences of numbers, because series are functions. \\

% A \textbf{Taylor Series} is an extremely useful series for approximating the relationship of sequence of numbers (data is a sequence of numbers). Taylor series underpins a lot of the math we do today, particularly for any relationship that isn't linear. 

% \begin{align}
%     f(x) = f(a) + \frac{f'(a)}{1!}(x-a)^1 + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + ... + \frac{f^k(a)}{k!}(x-a)^k + R
% \end{align}
% where $R$ is the residual.\\


% \subsection{0th order Taylor Series}
% Consider a 0th order taylor series: it would be a mean aka just a flat line equal to $f(a)$. \\

% If you're only looking at the means of a dataset, that means you're doing a 0th order taylor approximation.

% \subsection{First order Taylor Series}

% Consider a first order taylor series: 
% \begin{align}
%     f(x) &= f(a) + \frac{f'(a)}{1!}(x-a)^1 \\
%     &= A + B (x-a) \\
%     &= A -aB + Bx\\
%     &= m + Bx
% \end{align}
% It's a line with a slope. A first order Taylor series is a linear approximation. Linear regression is a first order Taylor approximation. \\
 

\end{document}